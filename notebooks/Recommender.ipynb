{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from sklearn.model_selection import train_test_split\n",
    "from surprise import (\n",
    "    NMF,\n",
    "    SVD,\n",
    "    BaselineOnly,\n",
    "    CoClustering,\n",
    "    Dataset,\n",
    "    KNNBaseline,\n",
    "    KNNBasic,\n",
    "    KNNWithMeans,\n",
    "    KNNWithZScore,\n",
    "    NormalPredictor,\n",
    "    Reader,\n",
    "    SlopeOne,\n",
    "    SVDpp,\n",
    "    accuracy,\n",
    ")\n",
    "from surprise.model_selection import GridSearchCV, KFold, cross_validate\n",
    "from transformers import pipeline\n",
    "\n",
    "from recommender.utils.collaborative import get_Iu, get_Ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = os.getenv(\"MONGO_URI\")\n",
    "PINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = \"recommender-system\"\n",
    "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "gpt4all_kwargs = {\"allow_download\": \"True\"}\n",
    "model_path = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "RANDOM_STATE = 101\n",
    "MLFLOW_TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\")\n",
    "MLFLOW_EXPERIMENT_NAME = os.getenv(\"MLFLOW_EXPERIMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = MongoClient(MONGO_URI)\n",
    "db = connection[\"shein-mirror\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = db[\"product_reviews\"]\n",
    "data = pd.DataFrame(list(input_data.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"rating\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", model=model_path, tokenizer=model_path, device=\"mps\", batch_size=8, truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sentiment_pipeline(data[\"review\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sentiment\"] = [int(r[\"label\"][0:1]) for r in res]\n",
    "data[\"sentiment_score\"] = [r[\"score\"] for r in res]\n",
    "data[\"rating_from_score\"] = np.round(data[\"sentiment_score\"] * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sentiment\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"rating_from_score\"].plot(kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(\"data/processed/reviews.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(\"../data/processed/reviews.parquet\", engine=\"pyarrow\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data, x=\"rating\", color=\"rating\", title=\"Rating distribution - Before sentiment analysis\", text_auto=True\n",
    ")\n",
    "\n",
    "with open(\"../img/charts/rating_distribution_before_sentiment.json\", \"w\") as f:\n",
    "    f.write(fig.to_json())\n",
    "# fig.to_json(\"img/charts/rating_distribution_before_sentiment.json\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"rating\"] = data[\"sentiment\"].apply(lambda x: int(x[0:1]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    data, x=\"rating\", color=\"rating\", title=\"Rating distribution - After sentiment analysis\", text_auto=True\n",
    ")\n",
    "with open(\"img/charts/rating_distribution_after_sentiment.json\", \"w\") as f:\n",
    "    f.write(fig.to_json())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db[\"product_reviews-mirror\"].drop()\n",
    "# db[\"product_reviews-mirror\"].insert_many(data.to_dict(orient=\"records\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"nickname\", \"product_id\", \"rating\"]]\n",
    "data = data.rename(columns={\"nickname\": \"userID\", \"product_id\": \"itemID\", \"sentiment\": \"rating\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = list(data[\"userID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = list(data[\"itemID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.3, random_state=RANDOM_STATE, stratify=data[[\"rating\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "data_sp = Dataset.load_from_df(data, reader=reader)\n",
    "train_sp = Dataset.load_from_df(train, reader=reader)\n",
    "test_sp = Dataset.load_from_df(test, reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = train_sp.build_full_trainset()\n",
    "testset = test_sp.build_full_trainset().build_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = [\n",
    "    SVD(random_state=RANDOM_STATE),\n",
    "    BaselineOnly(),\n",
    "    CoClustering(),\n",
    "    KNNBaseline(),\n",
    "    KNNWithZScore(),\n",
    "    KNNWithMeans(),\n",
    "    SlopeOne(),\n",
    "    KNNBasic(),\n",
    "    NormalPredictor(),\n",
    "    NMF(random_state=RANDOM_STATE),\n",
    "    SVDpp(random_state=RANDOM_STATE),\n",
    "]\n",
    "names = [algo[i].__class__.__name__ for i in range(len(algo))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {names[i]: {\"algo\": algo[i]} for i in range(len(algo))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.zeros((len(names), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Default parameters\") as run:\n",
    "    experiment_id = run.info.experiment_id\n",
    "    for k, v in algos.items():\n",
    "        with mlflow.start_run(experiment_id=experiment_id, run_name=k, nested=True) as subruns:\n",
    "            tab = cross_validate(v[\"algo\"], data_sp, cv=5, verbose=True, n_jobs=-1)\n",
    "            v[\"test_RMSE\"] = tab[\"test_rmse\"]\n",
    "            v[\"test_mae\"] = tab[\"test_mae\"]\n",
    "            v[\"fit_time\"] = tab[\"fit_time\"]\n",
    "            v[\"test_time\"] = tab[\"test_time\"]\n",
    "\n",
    "            rmse = np.mean(tab[\"test_rmse\"])\n",
    "            mae = np.mean(tab[\"test_mae\"])\n",
    "            ft = np.mean(tab[\"fit_time\"])\n",
    "            tt = np.mean(tab[\"test_time\"])\n",
    "\n",
    "            mlflow.log_metrics({\"rmse_test\": rmse, \"mae_test\": mae, \"fit_time\": ft, \"test_time\": tt})\n",
    "            mlflow.sklearn.log_model(v[\"algo\"], k)\n",
    "\n",
    "            result[algo.index(v[\"algo\"])] = [rmse, mae, ft, tt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for k, v in algos.items():\n",
    "    plt.boxplot(\n",
    "        v[\"test_RMSE\"],\n",
    "        positions=[list(algos.keys()).index(k)],\n",
    "        widths=0.6,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(\n",
    "            facecolor=\"C\" + str(list(algos.keys()).index(k)), color=\"C\" + str(list(algos.keys()).index(k)), linewidth=2\n",
    "        ),\n",
    "    )\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.xticks(range(len(algos)), list(algos.keys()), rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos_df = pd.DataFrame(algos)\n",
    "algos_df = algos_df.T\n",
    "algos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for k, v in algos.items():\n",
    "    plt.boxplot(\n",
    "        v[\"test_mae\"],\n",
    "        positions=[list(algos.keys()).index(k)],\n",
    "        widths=0.6,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(\n",
    "            facecolor=\"C\" + str(list(algos.keys()).index(k)), color=\"C\" + str(list(algos.keys()).index(k)), linewidth=2\n",
    "        ),\n",
    "    )\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.xticks(range(len(algos)), list(algos.keys()), rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for k, v in algos.items():\n",
    "    plt.boxplot(\n",
    "        v[\"fit_time\"],\n",
    "        positions=[list(algos.keys()).index(k)],\n",
    "        widths=0.6,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(\n",
    "            facecolor=\"C\" + str(list(algos.keys()).index(k)), color=\"C\" + str(list(algos.keys()).index(k)), linewidth=2\n",
    "        ),\n",
    "    )\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.xticks(range(len(algos)), list(algos.keys()), rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for k, v in algos.items():\n",
    "    plt.boxplot(\n",
    "        v[\"test_time\"],\n",
    "        positions=[list(algos.keys()).index(k)],\n",
    "        widths=0.6,\n",
    "        patch_artist=True,\n",
    "        boxprops=dict(\n",
    "            facecolor=\"C\" + str(list(algos.keys()).index(k)), color=\"C\" + str(list(algos.keys()).index(k)), linewidth=2\n",
    "        ),\n",
    "    )\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.xticks(range(len(algos)), list(algos.keys()), rotation=45)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.round(3)\n",
    "result = pd.DataFrame(result, index=list(algos.keys()), columns=[\"RMSE\", \"MAE\", \"fit_time\", \"test_time\"])\n",
    "result = result.sort_values(by=\"RMSE\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(\n",
    "    result,\n",
    "    labels=dict(x=\"Metrics\", y=\"Algorithms\", color=\"Values\"),\n",
    "    x=[\"RMSE\", \"MAE\", \"fit_time\", \"test_time\"],\n",
    "    y=result.index,\n",
    "    text_auto=True,\n",
    "    aspect=\"auto\",\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    origin=\"lower\",\n",
    "    title=\"Algorithms performance\",\n",
    ")\n",
    "with open(\"img/charts/result_metrics.json\", \"w\") as f:\n",
    "    f.write(fig.to_json())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svd = {\n",
    "    \"n_epochs\": [5, 10, 20],\n",
    "    \"lr_all\": [0.002, 0.005],\n",
    "    \"reg_all\": [0.4, 0.6],\n",
    "    \"n_factors\": [15, 30, 100],\n",
    "    \"random_state\": [RANDOM_STATE],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_bso = {\"bsl_options\": {\"method\": [\"als\", \"sgd\"], \"n_epochs\": [5, 15], \"reg_u\": [10, 20], \"reg_i\": [5, 15]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {\n",
    "    \"SVD\": {\"algo\": SVD, \"params\": param_grid_svd},\n",
    "    \"BaselineOnly\": {\"algo\": BaselineOnly, \"params\": param_grid_bso},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Fine-tuned-selected\") as run:\n",
    "    experiment_id = run.info.experiment_id\n",
    "    for k, v in algos.items():\n",
    "        print(k)\n",
    "        with mlflow.start_run(experiment_id=experiment_id, run_name=k, nested=True) as subruns:\n",
    "            gs = GridSearchCV(v[\"algo\"], v[\"params\"], measures=[\"rmse\", \"mae\"], cv=5, n_jobs=-1)\n",
    "            gs.fit(train_sp)\n",
    "\n",
    "            mlflow.log_params(gs.best_params[\"rmse\"])\n",
    "            mlflow.log_metrics({\"rmse\": gs.best_score[\"rmse\"], \"mae\": gs.best_score[\"mae\"]})\n",
    "\n",
    "            algo = v[\"algo\"](**gs.best_params[\"rmse\"])\n",
    "            algo.fit(trainset)\n",
    "            predictions = algo.test(testset)\n",
    "            rmse = accuracy.rmse(predictions)\n",
    "            mae = accuracy.mae(predictions)\n",
    "\n",
    "            mlflow.log_metrics({\"rmse_test\": rmse, \"mae_test\": mae})\n",
    "\n",
    "            mlflow.sklearn.log_model(algo, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_train = algo.predict(trainset.build_testset())\n",
    "predictions_test = algo.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy.rmse(predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea la matriz de predicciones\n",
    "n = len(users)\n",
    "m = len(products)\n",
    "recomendation = np.zeros((n, m))\n",
    "\n",
    "for k in users:\n",
    "    u = users.index(k)\n",
    "    for product_id in products:\n",
    "        i = products.index(product_id)\n",
    "        recomendation[u, i] = algo.predict(k, product_id, verbose=False)[3]\n",
    "\n",
    "recomendation = pd.DataFrame(recomendation, index=users, columns=products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3)\n",
    "bsl_options = {\"method\": \"als\", \"n_epochs\": 5, \"reg_u\": 12, \"reg_i\": 5}\n",
    "\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "for trainset, testset in kf.split(data_sp):\n",
    "    # train and test algorithm.\n",
    "    algo.fit(trainset)\n",
    "    predictions = algo.test(testset)\n",
    "    # Compute and print Root Mean Squared Error\n",
    "    accuracy.rmse(predictions, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(data_sp.build_full_trainset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "cross_validate(algo, data_sp, measures=[\"RMSE\", \"MAE\"], cv=3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data_sp, test_size=0.25)\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "predictions = algo.fit(trainset).test(testset)\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(predictions, columns=[\"uid\", \"iid\", \"rui\", \"est\", \"details\"])\n",
    "df[\"Iu\"] = df.uid.apply(get_Iu)\n",
    "df[\"Ui\"] = df.iid.apply(get_Ui)\n",
    "df[\"err\"] = abs(df.est - df.rui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"Iu\",\n",
    "    y=\"err\",\n",
    "    title=\"Error vs. Number of items rated by user\",\n",
    "    labels={\"Iu\": \"Number of items rated by user\", \"err\": \"Error\"},\n",
    ")\n",
    "\n",
    "with open(\"img/charts/error_vs_number_of_items_rated_by_user.json\", \"w\") as f:\n",
    "    f.write(fig.to_json())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    df, x=\"err\", title=\"Error distribution\", text_auto=False, labels={\"err\": \"Error\"}, histnorm=\"probability density\"\n",
    ")\n",
    "with open(\"img/charts/error_distribution.json\", \"w\") as f:\n",
    "    f.write(fig.to_json())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line()\n",
    "fig.add_scatter(\n",
    "    x=df.groupby(\"Iu\", as_index=False)[\"err\"].min()[\"Iu\"],\n",
    "    y=df.groupby(\"Iu\", as_index=False)[\"err\"].min()[\"err\"],\n",
    "    mode=\"lines\",\n",
    "    name=\"min\",\n",
    "    line_shape=\"spline\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df.groupby(\"Iu\", as_index=False)[\"err\"].mean()[\"Iu\"],\n",
    "    y=df.groupby(\"Iu\", as_index=False)[\"err\"].mean()[\"err\"],\n",
    "    mode=\"lines\",\n",
    "    name=\"mean\",\n",
    "    line_shape=\"spline\",\n",
    ")\n",
    "fig.add_scatter(\n",
    "    x=df.groupby(\"Iu\", as_index=False)[\"err\"].max()[\"Iu\"],\n",
    "    y=df.groupby(\"Iu\", as_index=False)[\"err\"].max()[\"err\"],\n",
    "    mode=\"lines\",\n",
    "    name=\"max\",\n",
    "    line_shape=\"spline\",\n",
    ")\n",
    "fig.layout.title = \"Error vs. Number of items rated by user\"\n",
    "fig.layout.xaxis.title = \"Number of items rated by user\"\n",
    "fig.layout.yaxis.title = \"Error\"\n",
    "fig.layout.legend.title = \"Error\"\n",
    "\n",
    "with open(\"img/charts/error_vs_number_of_items_rated_by_user.json\", \"w\") as f:\n",
    "    f.write(fig.to_json())\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
